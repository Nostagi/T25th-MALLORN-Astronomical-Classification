{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13903918,"sourceType":"datasetVersion","datasetId":8858410}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport os\nimport warnings\nimport copy\nfrom scipy.stats import skew, kurtosis, median_abs_deviation, linregress\nfrom scipy.signal import find_peaks, lombscargle\nfrom scipy.optimize import curve_fit\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve, auc\nfrom sklearn.inspection import permutation_importance\n\n# Xử lý tương thích phiên bản scipy cho hàm tích phân\ntry:\n    from scipy.integrate import trapezoid as trapz\nexcept ImportError:\n    try:\n        from scipy.integrate import trapz\n    except ImportError:\n        from numpy import trapz\n\n# Cấu hình môi trường\nwarnings.filterwarnings('ignore')\nBASE_PATH = \"/kaggle/input/astronomical\"\nTRAIN_LOG_PATH = f\"{BASE_PATH}/train_log.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.405387Z","iopub.execute_input":"2025-12-24T13:50:21.406159Z","iopub.status.idle":"2025-12-24T13:50:21.411350Z","shell.execute_reply.started":"2025-12-24T13:50:21.406134Z","shell.execute_reply":"2025-12-24T13:50:21.410639Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ==========================================\n# CÁC HÀM HỖ TRỢ TÍNH TOÁN (STATISTICS & PHYSICS)\n# ==========================================\n\ndef calculate_peak_lags(band_max_times):\n    \"\"\"Tính độ trễ thời gian giữa đỉnh sáng của các băng tần.\"\"\"\n    lags = {}\n    pairs = [('u', 'g'), ('g', 'r'), ('r', 'i')]\n    \n    for b1, b2 in pairs:\n        if (b1 in band_max_times) and (b2 in band_max_times):\n            lag = band_max_times[b1] - band_max_times[b2]\n            lags[f'lag_{b1}_{b2}'] = lag\n        else:\n            lags[f'lag_{b1}_{b2}'] = 0\n    return lags\n\ndef calculate_color_slope(time, flux, bands):\n    \"\"\"Tính độ dốc thay đổi màu (g - r) theo thời gian.\"\"\"\n    try:\n        mask_g = (bands == 'g')\n        mask_r = (bands == 'r')\n        if (np.sum(mask_g) < 2) or (np.sum(mask_r) < 2): \n            return 0\n        \n        df_g = pd.DataFrame({'t': time[mask_g], 'g': flux[mask_g]}).sort_values('t')\n        df_r = pd.DataFrame({'t': time[mask_r], 'r': flux[mask_r]}).sort_values('t')\n        \n        # Merge dữ liệu g và r với sai số thời gian cho phép là 1 ngày\n        df_merged = pd.merge_asof(df_g, df_r, on='t', tolerance=1, direction='nearest').dropna()\n        \n        if len(df_merged) < 3: \n            return 0\n        \n        df_merged['color'] = df_merged['g'] - df_merged['r']\n        slope, _, _, _, _ = linregress(df_merged['t'], df_merged['color'])\n        return slope\n    except:\n        return 0\n\ndef calculate_advanced_stats(flux, flux_err):\n    \"\"\"Tính các chỉ số thống kê mô tả phân phối flux.\"\"\"\n    if len(flux) < 2: \n        keys = ['iqr', 'variability_index', 'norm_mad', 'excess_variance', 'flux_ratio_95_5', 'beyond_1std']\n        return {k: 0 for k in keys}\n\n    flux = np.nan_to_num(flux, nan=np.nanmedian(flux))\n    \n    q75, q25 = np.percentile(flux, [75, 25])\n    iqr = q75 - q25\n    std_val = np.std(flux)\n    mean_val = np.mean(flux)\n    med_val = np.median(flux)\n    \n    var_index = (np.max(flux) - np.min(flux)) / (std_val + 1e-9)\n    \n    mad_val = median_abs_deviation(flux)\n    norm_mad = mad_val / (np.abs(med_val) + 1e-9)\n    \n    mean_sq_err = np.mean(flux_err**2)\n    excess_var = (np.var(flux) - mean_sq_err) / (mean_val**2 + 1e-9)\n    \n    q95, q05 = np.percentile(flux, [95, 5])\n    flux_ratio_95_5 = q95 / (abs(q05) + 1e-9)\n    \n    beyond_1std = np.sum(np.abs(flux - mean_val) > std_val) / len(flux)\n    \n    return {\n        'iqr': iqr,\n        'variability_index': var_index,\n        'norm_mad': norm_mad,\n        'excess_variance': max(0, excess_var),\n        'flux_ratio_95_5': flux_ratio_95_5,\n        'beyond_1std': beyond_1std\n    }\n\ndef calculate_variability_features(flux, time):\n    \"\"\"Tính các đặc trưng biến thiên chuỗi thời gian (Von Neumann, Periodogram).\"\"\"\n    if len(flux) < 3:\n        return {'von_neumann_eta': 0, 'mean_deriv1': 0, 'std_deriv1': 0, \n                'num_peaks': 0, 'period_max_power': 0, 'period_freq_at_max': 0}\n    \n    eta = np.mean(np.diff(flux)**2) / (np.var(flux) + 1e-9)\n    \n    dt = np.diff(time)\n    mask_dt = dt > 0.001\n    if np.any(mask_dt):\n        deriv = np.diff(flux)[mask_dt] / dt[mask_dt]\n        mean_deriv = np.mean(deriv)\n        std_deriv = np.std(deriv)\n    else:\n        mean_deriv, std_deriv = 0, 0\n        \n    peaks, _ = find_peaks(flux)\n    num_peaks = len(peaks)\n    \n    max_power, freq_at_max = 0, 0\n    try:\n        freqs = np.linspace(0.01, 5, 30)\n        pgram = lombscargle(time, flux, freqs * 2 * np.pi, normalize=True)\n        max_power = np.max(pgram)\n        freq_at_max = freqs[np.argmax(pgram)]\n    except: \n        pass\n    \n    return {\n        'von_neumann_eta': eta,\n        'mean_deriv1': mean_deriv,\n        'std_deriv1': std_deriv,\n        'num_peaks': num_peaks,\n        'period_max_power': max_power,\n        'period_freq_at_max': freq_at_max\n    }\n\ndef bazin_func(time, A, B, t0, t_fall, t_rise):\n    \"\"\"Mô hình Bazin function cho Light curve.\"\"\"\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        val = A * (np.exp(-(time - t0) / t_fall) / (1 + np.exp((time - t0) / t_rise))) + B\n    return np.nan_to_num(val)\n\ndef calculate_bazin_features(time, flux):\n    \"\"\"Fit Bazin function và trả về tham số.\"\"\"\n    if len(time) < 5: \n        return np.nan, np.nan, np.nan\n    try:\n        peak_idx = np.argmax(flux)\n        p0 = [np.max(flux), np.min(flux), time[peak_idx], 50, 10]\n        bounds = ([0, -np.inf, -np.inf, 1e-3, 1e-3], [np.inf, np.inf, np.inf, 1000, 500])\n        popt, _ = curve_fit(bazin_func, time, flux, p0=p0, bounds=bounds, maxfev=800)\n        \n        flux_pred = bazin_func(time, *popt)\n        rmse = np.sqrt(np.mean((flux - flux_pred)**2)) / (np.std(flux) + 1e-9)\n        \n        return popt[4], popt[3], rmse # t_rise, t_fall, rmse\n    except:\n        return np.nan, np.nan, np.nan\n\ndef calculate_decay_slope(time, flux, time_peak):\n    \"\"\"Tính độ dốc giai đoạn giảm sáng (post-peak).\"\"\"\n    mask = (time > time_peak) & (flux > 0)\n    t_post = time[mask]\n    f_post = flux[mask]\n    if len(t_post) < 3: return np.nan\n    try:\n        dt = t_post - time_peak + 1\n        slope, _, _, _, _ = linregress(np.log(dt), np.log(f_post))\n        return -slope\n    except: return np.nan\n\ndef calculate_stetson_j_k(flux, flux_err):\n    \"\"\"Tính chỉ số Stetson J và K.\"\"\"\n    n = len(flux)\n    if n < 2: return 0, 0\n    mean_f = np.mean(flux)\n    delta = np.sqrt(n / (n - 1)) * (flux - mean_f) / (flux_err + 1e-9)\n    K = (1/np.sqrt(n)) * np.sum(np.abs(delta)) / np.sqrt(np.sum(delta**2))\n    P_k = delta**2 - 1\n    J = (1/n) * np.sum(np.sign(P_k) * np.sqrt(np.abs(P_k)))\n    return J, K\n\ndef exponential_func(time, A, t0, tau):\n    \"\"\"Mô hình phân rã mũ (Exponential Decay).\"\"\"\n    with np.errstate(over='ignore', invalid='ignore'):\n        val = np.where(time > t0, A * np.exp(-(time - t0) / (tau + 1e-5)), 0)\n    return np.nan_to_num(val)\n\ndef calculate_model_comparison(time, flux, bazin_rmse):\n    \"\"\"So sánh sai số giữa mô hình Bazin (đặc trưng TDE) và Exponential (đặc trưng SN).\"\"\"\n    if len(time) < 5: return np.nan\n    try:\n        peak_idx = np.argmax(flux)\n        p0 = [np.max(flux), time[peak_idx], 20]\n        bounds = ([0, -np.inf, 1e-3], [np.inf, np.inf, 500])\n        \n        popt, _ = curve_fit(exponential_func, time, flux, p0=p0, bounds=bounds, maxfev=800)\n        flux_pred = exponential_func(time, *popt)\n        sn_rmse = np.sqrt(np.mean((flux - flux_pred)**2)) / (np.std(flux) + 1e-9)\n        \n        return bazin_rmse / (sn_rmse + 1e-9)\n    except:\n        return np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.420801Z","iopub.execute_input":"2025-12-24T13:50:21.421388Z","iopub.status.idle":"2025-12-24T13:50:21.442872Z","shell.execute_reply.started":"2025-12-24T13:50:21.421370Z","shell.execute_reply":"2025-12-24T13:50:21.442366Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ==========================================\n# TRÍCH XUẤT ĐẶC TRƯNG (FEATURE EXTRACTION)\n# ==========================================\n\ndef extract_features(df_chunk):\n    features_list = []\n    grouped = df_chunk.groupby('object_id')\n    \n    for obj_id, group in grouped:\n        group = group.sort_values('mjd')\n        \n        flux = group['flux'].values\n        mjd = group['mjd'].values\n        bands = group['band'].values\n        flux_err = group['flux_err'].values if 'flux_err' in group.columns else np.ones_like(flux)\n        \n        # 1. Global Features\n        stats = {\n            'object_id': obj_id,\n            'flux_min': np.min(flux),\n            'flux_max': np.max(flux),\n            'flux_mean': np.mean(flux),\n            'flux_median': np.median(flux),\n            'flux_std': np.std(flux),\n            'flux_skew': skew(flux),\n            'flux_kurtosis': kurtosis(flux),\n            'flux_mad': median_abs_deviation(flux),\n            'flux_amplitude': (np.max(flux) - np.min(flux))/2,\n            'duration': mjd[-1] - mjd[0]\n        }\n        \n        try:\n            stats['flux_integral_global'] = trapz(y=flux, x=mjd)\n        except: \n            stats['flux_integral_global'] = 0\n\n        cusum = np.cumsum(flux - np.mean(flux))\n        stats['cusum_range'] = np.max(cusum) - np.min(cusum)\n        \n        stats.update(calculate_variability_features(flux, mjd))\n        \n        _, k_global = calculate_stetson_j_k(flux, flux_err)\n        stats['stetson_k'] = k_global\n\n        # Weighted Time Statistics\n        pos_mask = flux > 0\n        if np.sum(pos_mask) > 2:\n            t_w = mjd[pos_mask]; f_w = flux[pos_mask]\n            w_mean = np.average(t_w, weights=f_w)\n            w_std = np.sqrt(np.average((t_w - w_mean)**2, weights=f_w))\n            stats['time_width_weighted'] = w_std\n            stats['time_skew_weighted'] = np.average(((t_w - w_mean)/(w_std+1e-9))**3, weights=f_w)\n        else:\n            stats['time_width_weighted'] = 0; stats['time_skew_weighted'] = 0\n\n        # Rise/Fall Time\n        peak_idx = np.argmax(flux)\n        time_peak = mjd[peak_idx]\n        stats['rise_time'] = time_peak - mjd[0]\n        stats['fall_time'] = mjd[-1] - time_peak\n        stats['rise_fall_ratio'] = stats['rise_time'] / (stats['fall_time'] + 1e-9)\n        \n        # Linear Slopes\n        try:\n            stats['slope_rise_global'], _, _, _, _ = linregress(mjd[:peak_idx+1], flux[:peak_idx+1]) if peak_idx > 1 else (0,0,0,0,0)\n            stats['slope_fall_global'], _, _, _, _ = linregress(mjd[peak_idx:], flux[peak_idx:]) if len(mjd)-peak_idx > 1 else (0,0,0,0,0)\n        except: \n            stats['slope_rise_global'] = 0; stats['slope_fall_global'] = 0\n            \n        stats['percent_time_above_mean'] = np.sum(flux > np.mean(flux)) / len(flux)\n        stats['color_slope_g_r'] = calculate_color_slope(mjd, flux, bands)\n\n        # 2. Band-specific Features\n        band_max_flux = {} \n        band_max_times = {}\n\n        for b in ['u', 'g', 'r', 'i', 'z']:\n            mask = (bands == b)\n            \n            if np.sum(mask) > 0:\n                f_b = flux[mask]; t_b = mjd[mask]; err_b = flux_err[mask]\n                \n                stats[f'mean_{b}'] = np.mean(f_b)\n                stats[f'max_{b}'] = np.max(f_b)\n                stats[f'min_{b}'] = np.min(f_b)\n                stats[f'std_{b}'] = np.std(f_b)\n                stats[f'amp_{b}'] = np.max(f_b) - np.min(f_b)\n                \n                band_max_flux[b] = np.max(f_b)\n                idx_max_b = np.argmax(f_b)\n                band_max_times[b] = t_b[idx_max_b]\n                \n                try:\n                    stats[f'integral_{b}'] = trapz(y=f_b, x=t_b)\n                except:\n                    stats[f'integral_{b}'] = 0\n                \n                adv_stats = calculate_advanced_stats(f_b, err_b)\n                for k, v in adv_stats.items():\n                    stats[f'{k}_{b}'] = v\n                \n                J, K = calculate_stetson_j_k(f_b, err_b)\n                stats[f'stetson_J_{b}'] = J\n                stats[f'stetson_K_{b}'] = K\n                stats[f'von_neumann_{b}'] = np.mean(np.diff(f_b)**2)/(np.var(f_b)+1e-9) if len(f_b)>2 else 0\n                \n                p05, p95 = np.percentile(f_b, [5, 95]) if len(f_b)>1 else (f_b[0], f_b[0])\n                stats[f'p05_{b}'] = p05; stats[f'p95_{b}'] = p95\n                stats[f'flux_width_90_{b}'] = p95 - p05\n                try: \n                    stats[f'linear_slope_{b}'] = linregress(t_b, f_b)[0]\n                except: \n                    stats[f'linear_slope_{b}'] = 0\n                \n                # Model Fitting (Bazin & Decay)\n                if b in ['u', 'g', 'r']:\n                    t_rise_bz, t_fall_bz, bazin_rmse = calculate_bazin_features(t_b, f_b)\n                    \n                    stats[f'bazin_rise_{b}'] = t_rise_bz\n                    stats[f'bazin_fall_{b}'] = t_fall_bz\n                    stats[f'bazin_rmse_{b}'] = bazin_rmse\n                    \n                    if not np.isnan(bazin_rmse):\n                        stats[f'chi2_ratio_{b}'] = calculate_model_comparison(t_b, f_b, bazin_rmse)\n                    else:\n                        stats[f'chi2_ratio_{b}'] = np.nan\n                    \n                    t_peak_b = t_b[np.argmax(f_b)]\n                    stats[f'decay_slope_{b}'] = calculate_decay_slope(t_b, f_b, t_peak_b)\n\n            else:\n                band_max_flux[b] = 0\n                stats[f'integral_{b}'] = 0\n                \n                miss_cols = ['mean', 'max', 'min', 'std', 'amp', 'iqr', 'variability_index', \n                             'norm_mad', 'excess_variance', 'flux_ratio_95_5', 'beyond_1std',\n                             'stetson_J', 'stetson_K', 'von_neumann', 'p05', 'p95', \n                             'flux_width_90', 'linear_slope']\n                for c in miss_cols: stats[f'{c}_{b}'] = 0\n                \n                if b in ['u', 'g', 'r']:\n                    stats[f'bazin_rise_{b}'] = np.nan\n                    stats[f'bazin_fall_{b}'] = np.nan\n                    stats[f'bazin_rmse_{b}'] = np.nan\n                    stats[f'decay_slope_{b}'] = np.nan\n                    stats[f'chi2_ratio_{b}'] = np.nan\n\n        # 3. Inter-band Features\n        stats['color_g_r'] = stats['mean_g'] - stats['mean_r']\n        stats['color_u_z'] = stats['mean_u'] - stats['mean_z']\n        \n        stats['flux_ratio_u_g'] = band_max_flux['u'] / (band_max_flux['g'] + 1e-9)\n        stats['flux_ratio_g_r'] = band_max_flux['g'] / (band_max_flux['r'] + 1e-9)\n        stats['flux_ratio_u_r'] = band_max_flux['u'] / (band_max_flux['r'] + 1e-9)\n        \n        lags = calculate_peak_lags(band_max_times)\n        stats.update(lags)\n        \n        features_list.append(stats)\n        \n    return pd.DataFrame(features_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.563018Z","iopub.execute_input":"2025-12-24T13:50:21.563212Z","iopub.status.idle":"2025-12-24T13:50:21.581626Z","shell.execute_reply.started":"2025-12-24T13:50:21.563196Z","shell.execute_reply":"2025-12-24T13:50:21.580812Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ==========================================\n# POST-PROCESSING: PHYSICS ESTIMATION\n# ==========================================\n\ndef planck_func(wav_angstrom, T):\n    h = 6.626e-34; c = 3.0e8; k = 1.38e-23\n    w = wav_angstrom * 1e-10\n    if T < 100: T = 100 \n    a = 2.0 * h * c**2\n    b = (h * c) / (w * k * T)\n    with np.errstate(over='ignore', invalid='ignore'):\n        val = a / ( (w**5) * (np.exp(b) - 1.0) )\n    return np.nan_to_num(val)\n\ndef estimate_temperature(row):\n    WAVELENGTHS = {'u': 3685, 'g': 4802, 'r': 6231, 'i': 7542, 'z': 8690}\n    wavs = []; fluxes = []\n    for b, w in WAVELENGTHS.items():\n        if row.get(f'max_{b}_corr', 0) > 0:\n            wavs.append(w)\n            fluxes.append(row[f'max_{b}_corr'])\n    if len(fluxes) < 3: return np.nan\n    \n    wavs = np.array(wavs); fluxes = np.array(fluxes)\n    norm_flux = fluxes / np.max(fluxes)\n    def fit_func(w, T): return planck_func(w, T) / np.max(planck_func(w, T))\n    try:\n        popt, _ = curve_fit(fit_func, wavs, norm_flux, p0=[10000], bounds=([1000], [100000]))\n        return popt[0]\n    except: return np.nan\n\ndef add_physics_features(df):\n    \"\"\"Bổ sung các đặc trưng vật lý: Temperature, Radius, Rest-frame metrics.\"\"\"\n    df_out = df.copy()\n    if 'Z' not in df_out.columns: df_out['Z'] = -1\n    if 'EBV' not in df_out.columns: df_out['EBV'] = 0\n    \n    # 1. Extinction Correction\n    R_lambda = {'u': 4.239, 'g': 3.303, 'r': 2.285, 'i': 1.698, 'z': 1.263}\n    for b in ['u', 'g', 'r', 'i', 'z']:\n        A_lambda = R_lambda[b] * df_out['EBV'].fillna(0)\n        corr = 10 ** (0.4 * A_lambda)\n        if f'mean_{b}' in df_out.columns: df_out[f'mean_{b}_corr'] = df_out[f'mean_{b}'] * corr\n        if f'max_{b}' in df_out.columns: df_out[f'max_{b}_corr'] = df_out[f'max_{b}'] * corr\n\n    # 2. Temperature Estimation\n    print(\"Calculating blackbody temperature...\")\n    df_out['temp_blackbody'] = df_out.apply(estimate_temperature, axis=1)\n\n    # 3. Rest-frame adjustment\n    z_factor = 1 + df_out['Z'].clip(lower=0)\n    for col in ['duration', 'rise_time', 'fall_time', 'bazin_rise_g', 'bazin_fall_g', 'bazin_rise_r', 'bazin_fall_r']:\n        if col in df_out.columns:\n            df_out[f'{col}_rest'] = df_out[col] / z_factor\n\n    # 4. Absolute Magnitude & Luminosity Proxy\n    dist_mod = 5 * np.log10(df_out['Z'].clip(lower=0.001) + 1e-5)\n    for b in ['g', 'r']:\n        if f'max_{b}_corr' in df_out.columns:\n            df_out[f'abs_mag_{b}'] = -2.5*np.log10(df_out[f'max_{b}_corr'].clip(lower=1e-5)) - dist_mod\n\n    # 5. Photospheric Radius Proxy\n    if 'abs_mag_g' in df_out.columns and 'temp_blackbody' in df_out.columns:\n        df_out['log_radius_photosphere'] = -0.2 * df_out['abs_mag_g'] - 2 * np.log10(df_out['temp_blackbody'].clip(lower=100))\n    else:\n        df_out['log_radius_photosphere'] = 0\n\n    # 6. Temperature Change Rate\n    if 'color_evol_gr' in df_out.columns and 'duration_rest' in df_out.columns:\n        df_out['temp_change_rate'] = df_out['color_evol_gr'] / (df_out['duration_rest'] + 1e-5)\n    else:\n        df_out['temp_change_rate'] = 0\n\n    print(\"Dropping intermediate columns (Z, EBV)...\")\n    df_out = df_out.drop(columns=['Z', 'EBV'], errors='ignore')\n    \n    return df_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.582964Z","iopub.execute_input":"2025-12-24T13:50:21.583149Z","iopub.status.idle":"2025-12-24T13:50:21.601330Z","shell.execute_reply.started":"2025-12-24T13:50:21.583134Z","shell.execute_reply":"2025-12-24T13:50:21.600648Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def load_and_process_data():\n    print(\"Starting data loading and processing...\")\n    all_features = []\n    \n    # 1. Feature Extraction from splits\n    for i in range(1, 21):\n        file_path = f\"{BASE_PATH}/split_{i:02d}/train_full_lightcurves.csv\"\n        if os.path.exists(file_path):\n            if i % 5 == 0: print(f\"Processing split {i:02d}...\")\n            df = pd.read_csv(file_path)\n            df = df.rename(columns={'Flux': 'flux', 'Flux_err': 'flux_err', 'Filter': 'band', 'Time (MJD)': 'mjd'})\n            feats = extract_features(df)\n            all_features.append(feats)\n    \n    full_features = pd.concat(all_features, ignore_index=True)\n    \n    # 2. Merge Metadata\n    print(\"Merging with metadata...\")\n    train_log = pd.read_csv(TRAIN_LOG_PATH)\n    meta_cols = ['object_id', 'target', 'Z', 'EBV'] \n    available_cols = [c for c in meta_cols if c in train_log.columns]\n    \n    final_df = pd.merge(train_log[available_cols], full_features, on='object_id', how='right')\n    \n    # 3. Final Preprocessing\n    if 'Z' in final_df.columns: final_df['Z'] = final_df['Z'].fillna(-1)\n    if 'EBV' in final_df.columns: final_df['EBV'] = final_df['EBV'].fillna(0)\n\n    print(\"Computing physics features...\")\n    final_df = add_physics_features(final_df)\n    \n    # Fill NaN\n    numeric_cols = final_df.select_dtypes(include=[np.number]).columns\n    cols_to_fill = [c for c in numeric_cols if c not in ['target', 'object_id']]\n    for col in cols_to_fill:\n        if final_df[col].isnull().any():\n            final_df[col] = final_df[col].fillna(final_df[col].median())\n            \n    return final_df\n\ndef prepare_data_for_training(df):\n    X = df.drop(columns=['object_id', 'target'])\n    y = df['target']\n    feature_names = X.columns.tolist()\n    \n    print(f\"Total features: {len(feature_names)}\")\n    print(\"Applying StandardScaler...\")\n    scaler = StandardScaler()\n    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_names)\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    return X_train, X_val, y_train, y_val, feature_names, scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.602064Z","iopub.execute_input":"2025-12-24T13:50:21.602473Z","iopub.status.idle":"2025-12-24T13:50:21.620989Z","shell.execute_reply.started":"2025-12-24T13:50:21.602455Z","shell.execute_reply":"2025-12-24T13:50:21.620348Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train_model(X_train, y_train, X_val, y_val):\n    print(\"\\nTraining Model...\")\n    \n    scale_pos_weight = 10\n    print(f\"Using scale_pos_weight: {scale_pos_weight}\")\n    \n    model = xgb.XGBClassifier(\n        n_estimators=2000,\n        learning_rate=0.01,\n        max_depth=4,\n        min_child_weight=5,\n        gamma=1.5,             \n        subsample=0.8,\n        colsample_bytree=0.6,\n        scale_pos_weight=12,\n        \n        objective='binary:logistic',\n        eval_metric=['auc', 'logloss'],\n        tree_method='hist',\n        device='cuda',\n        random_state=42,\n    )\n    \n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=200\n    )\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.622351Z","iopub.execute_input":"2025-12-24T13:50:21.622544Z","iopub.status.idle":"2025-12-24T13:50:21.637937Z","shell.execute_reply.started":"2025-12-24T13:50:21.622530Z","shell.execute_reply":"2025-12-24T13:50:21.637341Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef optimize_threshold(y_val, y_pred_proba):\n    \"\"\"Tìm ngưỡng (threshold) tối ưu dựa trên F1-Score.\"\"\"\n    print(\"\\nOptimizing Threshold...\")\n    \n    thresholds = np.arange(0.01, 1.00, 0.01)\n    f1_scores = []\n    \n    for t in thresholds:\n        y_pred_temp = (y_pred_proba >= t).astype(int)\n        f1 = f1_score(y_val, y_pred_temp, pos_label=1)\n        f1_scores.append(f1)\n        \n    best_idx = np.argmax(f1_scores)\n    best_threshold = thresholds[best_idx]\n    best_f1 = f1_scores[best_idx]\n    \n    print(f\"Best Threshold: {best_threshold:.2f} | F1-Score: {best_f1:.4f}\")\n    return best_threshold\n\ndef analyze_and_filter_features(model, feature_names, threshold=0.003):\n    imp_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': model.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    zero_imp = imp_df[imp_df['Importance'] == 0]\n    print(f\"Features with 0 importance: {len(zero_imp)}\")\n    \n    print(f\"\\nTop 10 Important Features:\")\n    print(imp_df.head(10))\n    \n    selected_features = imp_df[imp_df['Importance'] >= threshold]['Feature'].tolist()\n    print(f\"Selected {len(selected_features)}/{len(feature_names)} features.\")\n    \n    return selected_features\n\ndef evaluate_model_comprehensive(model, X_val, y_val, feature_names, threshold=0.5):\n    print(f\"\\nEvaluating Model (Threshold = {threshold:.2f})...\")\n    \n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    y_pred = (y_pred_proba >= threshold).astype(int)\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_val, y_pred, target_names=['Non-TDE', 'TDE']))\n    \n    feat_imp = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': model.feature_importances_\n    }).sort_values('Importance', ascending=False).head(20)\n    \n    return feat_imp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.638531Z","iopub.execute_input":"2025-12-24T13:50:21.638685Z","iopub.status.idle":"2025-12-24T13:50:21.651234Z","shell.execute_reply.started":"2025-12-24T13:50:21.638672Z","shell.execute_reply":"2025-12-24T13:50:21.650674Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def run_iterative_strategy(X_train, y_train, X_val, y_val, X_test_full, feature_names, scaler):\n    \n    params = {\n        'n_estimators': 2000,\n        'learning_rate': 0.02, \n        'max_depth': 5,           \n        'subsample': 0.8,\n        'colsample_bytree': 0.6,\n        'gamma': 1.0,             \n        'scale_pos_weight': 12,   \n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'tree_method': 'hist',\n        'device': 'cuda',         \n        'random_state': 42\n    }\n    \n    print(\"\\nStarting Iterative Pseudo-Labeling Pipeline...\")\n\n    # 1. Prepare Data\n    X_train_df = pd.DataFrame(X_train, columns=feature_names)\n    X_val_df = pd.DataFrame(X_val, columns=feature_names)\n    X_test_df = pd.DataFrame(X_test_full, columns=feature_names)\n    \n    # Initial Feature Selection\n    sel_model = xgb.XGBClassifier(**{**params, 'n_estimators': 500})\n    sel_model.fit(X_train_df, y_train, eval_set=[(X_val_df, y_val)], verbose=False)\n    \n    imp_df = pd.DataFrame({'feature': feature_names, 'score': sel_model.feature_importances_})\n    top_feats = imp_df.sort_values('score', ascending=False).head(80)['feature'].tolist()\n    phy_feats = ['rise_time', 'fall_time', 'amp_g', 'amp_r', 'duration', 'temp_blackbody', 'chi2_ratio_g', 'bazin_rise_g', 'bazin_fall_g']\n    final_feats = list(set(top_feats + [f for f in phy_feats if f in feature_names]))\n    \n    print(f\"Selected {len(final_feats)} features for loop.\")\n    \n    X_tr_sub = X_train_df[final_feats]\n    X_va_sub = X_val_df[final_feats]\n    X_te_sub = X_test_df[final_feats]\n    \n    X_curr = pd.concat([X_tr_sub, X_va_sub], axis=0)\n    y_curr = np.concatenate([y_train, y_val])\n    \n    # 2. Iterative Loops\n    pseudo_thresholds = [0.95, 0.90] \n    \n    for i, thresh in enumerate(pseudo_thresholds):\n        print(f\"Loop {i+1}: Pseudo-Labeling (Threshold > {thresh})\")\n        \n        model = xgb.XGBClassifier(**params)\n        model.fit(X_curr, y_curr, verbose=False)\n        \n        probs = model.predict_proba(X_te_sub)[:, 1]\n        idx_new = np.where(probs > thresh)[0]\n        print(f\"Found {len(idx_new)} new pseudo-labels.\")\n        \n        if len(idx_new) > 0:\n            X_pseudo = X_te_sub.iloc[idx_new]\n            y_pseudo = np.ones(len(idx_new))\n            X_curr = pd.concat([X_curr, X_pseudo], axis=0)\n            y_curr = np.concatenate([y_curr, y_pseudo])\n        else:\n            print(\"No new samples found. Breaking loop.\")\n            break\n\n    # 3. Final Averaging\n    print(\"\\nFinal Loop: Seed Averaging (5 seeds)\")\n    seeds = [42, 2024, 888, 77, 123]\n    avg_preds = np.zeros(len(X_te_sub))\n    final_params = params.copy()\n    final_params['n_estimators'] = 3000 \n    \n    for s in seeds:\n        final_params['random_state'] = s\n        clf = xgb.XGBClassifier(**final_params)\n        clf.fit(X_curr, y_curr, verbose=False)\n        avg_preds += clf.predict_proba(X_te_sub)[:, 1]\n    \n    avg_preds /= len(seeds)\n    print(\"Pipeline completed.\")\n    \n    return avg_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.652008Z","iopub.execute_input":"2025-12-24T13:50:21.652268Z","iopub.status.idle":"2025-12-24T13:50:21.671516Z","shell.execute_reply.started":"2025-12-24T13:50:21.652240Z","shell.execute_reply":"2025-12-24T13:50:21.670972Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def prepare_test_features(save_path=\"test_features_processed.csv\"):\n    if os.path.exists(save_path):\n        print(f\"Loading processed file: {save_path}\")\n        return pd.read_csv(save_path)\n\n    print(\"Extracting test features from raw files...\")\n    aggregated_test = []\n    \n    for i in range(1, 21):\n        file_path = f\"{BASE_PATH}/split_{i:02d}/test_full_lightcurves.csv\"\n        if os.path.exists(file_path):\n            if i % 5 == 0: print(f\"Processing split {i:02d}...\")\n            df_chunk = pd.read_csv(file_path)\n            df_chunk = df_chunk.rename(columns={'Flux': 'flux', 'Flux_err': 'flux_err', 'Filter': 'band', 'Time (MJD)': 'mjd'})\n            feats = extract_features(df_chunk)\n            aggregated_test.append(feats)\n\n    if not aggregated_test:\n        print(\"Error: No test data found.\")\n        return None\n\n    test_features = pd.concat(aggregated_test, ignore_index=True)\n    \n    print(\"Merging metadata...\")\n    test_log_path = f\"{BASE_PATH}/test_log.csv\"\n    if os.path.exists(test_log_path):\n        test_log = pd.read_csv(test_log_path)\n        meta_cols = ['object_id', 'Z', 'EBV'] \n        available = [c for c in meta_cols if c in test_log.columns]\n        final_df = pd.merge(test_log[available], test_features, on='object_id', how='right')\n    else:\n        final_df = test_features\n        final_df['Z'] = -1; final_df['EBV'] = 0\n\n    if 'Z' in final_df.columns: final_df['Z'] = final_df['Z'].fillna(-1)\n    if 'EBV' in final_df.columns: final_df['EBV'] = final_df['EBV'].fillna(0)\n    \n    num_cols = final_df.select_dtypes(include=[np.number]).columns\n    cols_to_fill = [c for c in num_cols if c not in ['object_id', 'Z', 'EBV']]\n    for c in cols_to_fill:\n        if final_df[c].isnull().any(): final_df[c] = final_df[c].fillna(0)\n\n    print(\"Adding physics features...\")\n    final_df = add_physics_features(final_df)\n    \n    final_df.to_csv(save_path, index=False)\n    print(\"Test data preparation complete.\")\n    return final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.672151Z","iopub.execute_input":"2025-12-24T13:50:21.672412Z","iopub.status.idle":"2025-12-24T13:50:21.691371Z","shell.execute_reply.started":"2025-12-24T13:50:21.672395Z","shell.execute_reply":"2025-12-24T13:50:21.690740Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def predict_and_submit(model, test_df, feature_names, scaler, threshold=0.5, filename=\"submission.csv\"):\n    print(f\"Generating submission (Threshold={threshold})...\")\n    \n    model_features = getattr(model, 'feature_names_in_', feature_names)\n    scaler_features = getattr(scaler, 'feature_names_in_', feature_names)\n\n    df_for_scaler = test_df.copy()\n    missing_scaler = set(scaler_features) - set(df_for_scaler.columns)\n    for c in missing_scaler: df_for_scaler[c] = 0\n            \n    try:\n        X_full_raw = df_for_scaler[scaler_features]\n    except KeyError as e:\n        print(f\"Error: Missing features for scaler: {e}\")\n        return\n\n    X_full_scaled_np = scaler.transform(X_full_raw)\n    X_full_scaled_df = pd.DataFrame(X_full_scaled_np, columns=scaler_features)\n    X_final = X_full_scaled_df[model_features]\n\n    y_prob = model.predict_proba(X_final)[:, 1]\n    y_pred = (y_prob >= threshold).astype(int)\n\n    submission = pd.DataFrame({'object_id': test_df['object_id'], 'target': y_pred})\n    n_tde = submission['target'].sum()\n    print(f\"Result: {n_tde} TDEs / {len(submission)} ({n_tde/len(submission):.2%})\")\n    \n    submission.to_csv(filename, index=False)\n    print(f\"Saved: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.692027Z","iopub.execute_input":"2025-12-24T13:50:21.692245Z","iopub.status.idle":"2025-12-24T13:50:21.708968Z","shell.execute_reply.started":"2025-12-24T13:50:21.692222Z","shell.execute_reply":"2025-12-24T13:50:21.708352Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load & Prepare Data\n    df = load_and_process_data()\n    X_train, X_val, y_train, y_val, feat_names, scaler = prepare_data_for_training(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:50:21.709869Z","iopub.execute_input":"2025-12-24T13:50:21.710143Z","iopub.status.idle":"2025-12-24T13:56:50.001263Z","shell.execute_reply.started":"2025-12-24T13:50:21.710121Z","shell.execute_reply":"2025-12-24T13:56:50.000544Z"}},"outputs":[{"name":"stdout","text":"Starting data loading and processing...\nProcessing split 05...\nProcessing split 10...\nProcessing split 15...\nProcessing split 20...\nMerging with metadata...\nComputing physics features...\nCalculating blackbody temperature...\nDropping intermediate columns (Z, EBV)...\nTotal features: 168\nApplying StandardScaler...\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load processed test data\n    test_full_df = prepare_test_features(\"test_features_processed_v4.csv\")\n    \n    if hasattr(scaler, 'feature_names_in_'):\n        scaler_feats = scaler.feature_names_in_\n    else:\n        scaler_feats = feat_names\n    \n    df_test_aligned = test_full_df.copy()\n    for c in (set(scaler_feats) - set(df_test_aligned.columns)): \n        df_test_aligned[c] = 0\n        \n    X_test_scaled_np = scaler.transform(df_test_aligned[scaler_feats])\n    \n    # Run iterative pipeline\n    final_probs = run_iterative_strategy(\n        X_train, y_train, X_val, y_val, \n        X_test_scaled_np, \n        scaler_feats, \n        scaler\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:56:50.003405Z","iopub.execute_input":"2025-12-24T13:56:50.003635Z","iopub.status.idle":"2025-12-24T14:11:42.777608Z","shell.execute_reply.started":"2025-12-24T13:56:50.003617Z","shell.execute_reply":"2025-12-24T14:11:42.776945Z"}},"outputs":[{"name":"stdout","text":"Extracting test features from raw files...\nProcessing split 05...\nProcessing split 10...\nProcessing split 15...\nProcessing split 20...\nMerging metadata...\nAdding physics features...\nCalculating blackbody temperature...\nDropping intermediate columns (Z, EBV)...\nTest data preparation complete.\n\nStarting Iterative Pseudo-Labeling Pipeline...\nSelected 84 features for loop.\nLoop 1: Pseudo-Labeling (Threshold > 0.95)\nFound 25 new pseudo-labels.\nLoop 2: Pseudo-Labeling (Threshold > 0.9)\nFound 71 new pseudo-labels.\n\nFinal Loop: Seed Averaging (5 seeds)\nPipeline completed.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"if 'final_probs' in locals():\n    print(\"Processing final probabilities...\")\n    \n    raw_test_df = pd.read_csv(\"test_features_processed_v4.csv\")\n    thresholds = [0.18, 0.20, 0.22, 0.25, 0.30]\n    \n    print(\"\\nGenerating final submissions (Relaxed Physics Filter)...\")\n    \n    for t in thresholds:\n        preds = (final_probs >= t).astype(int)\n        \n        # Filter Logic: Only remove extreme outliers (Rise Time > 200 days rest-frame)\n        if 'bazin_rise_g' in raw_test_df.columns:\n            mask_bad_rise = (preds == 1) & (raw_test_df['bazin_rise_g'] > 200)\n            preds[mask_bad_rise] = 0\n            if mask_bad_rise.sum() > 0:\n                print(f\"[Thresh {t}] Removed {mask_bad_rise.sum()} outliers.\")\n        \n        filename = f\"submission_final_relax_{t:.2f}.csv\"\n        sub = pd.DataFrame({'object_id': raw_test_df['object_id'], 'target': preds})\n        sub.to_csv(filename, index=False)\n        \n        cnt = sub['target'].sum()\n        print(f\"File: {filename} | TDEs: {cnt} ({cnt/len(sub):.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:11:42.778347Z","iopub.execute_input":"2025-12-24T14:11:42.778587Z","iopub.status.idle":"2025-12-24T14:11:43.085211Z","shell.execute_reply.started":"2025-12-24T14:11:42.778557Z","shell.execute_reply":"2025-12-24T14:11:43.084515Z"}},"outputs":[{"name":"stdout","text":"Processing final probabilities...\n\nGenerating final submissions (Relaxed Physics Filter)...\n[Thresh 0.18] Removed 26 outliers.\nFile: submission_final_relax_0.18.csv | TDEs: 530 (7.43%)\n[Thresh 0.2] Removed 23 outliers.\nFile: submission_final_relax_0.20.csv | TDEs: 509 (7.13%)\n[Thresh 0.22] Removed 20 outliers.\nFile: submission_final_relax_0.22.csv | TDEs: 492 (6.90%)\n[Thresh 0.25] Removed 18 outliers.\nFile: submission_final_relax_0.25.csv | TDEs: 462 (6.48%)\n[Thresh 0.3] Removed 16 outliers.\nFile: submission_final_relax_0.30.csv | TDEs: 425 (5.96%)\n","output_type":"stream"}],"execution_count":25}]}